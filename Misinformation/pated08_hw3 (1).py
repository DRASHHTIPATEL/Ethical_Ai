# -*- coding: utf-8 -*-
"""pated08_hw3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RhlvATyx-YS2uiuM8n8UXe_p2FrOPwBf
"""

#ethical ai-hw3
#drashti patel

"""#Part 1: Supervised Learning
Logistic Regression
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, confusion_matrix
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Load dataset provided to us
# LOACALLY UPLOAD IT
train_df = pd.read_csv("/content/train_misinfo.csv")
test_df = pd.read_csv("/content/test_misinfo(in).csv").iloc[:, :2]

# preprocessing
def preprocess_data(df):
    df = df.dropna().reset_index(drop=True)

    # Converting laabels to numerical values
    df["label"] = pd.to_numeric(df["label"], errors='coerce')
    # Drop invalid labels without printing
    df = df.dropna(subset=["label"]).reset_index(drop=True)
    # Convertin labels to integer type
    df["label"] = df["label"].astype(int)

    # Ensure text column is of string type
    df["text"] = df["text"].astype(str)

    return df

train_df = preprocess_data(train_df)
test_df = preprocess_data(test_df)

# Printing label distribution
print("Train Label Distribution:")
print(train_df["label"].value_counts())
print("Test Label Distribution:")
print(test_df["label"].value_counts())

# Print label distribution to check for imbalance
print("Train Label Distribution:")
print(train_df["label"].value_counts())
print("Test Label Distribution:")
print(test_df["label"].value_counts())
# the data is imbalanced
# handled it by class_weights

# Using TF-IDF vectors
vectorizer = TfidfVectorizer(max_features=5000)
X_train_tfidf = vectorizer.fit_transform(train_df["text"])
X_test_tfidf = vectorizer.transform(test_df["text"])
y_train = train_df["label"].values
y_test = test_df["label"].values

# Training
log_reg = LogisticRegression(C=0.1, max_iter=500, class_weight='balanced')
log_reg.fit(X_train_tfidf, y_train)

# Evaluation
train_acc = log_reg.score(X_train_tfidf, y_train)
test_acc = log_reg.score(X_test_tfidf, y_test)

y_pred = log_reg.predict(X_test_tfidf)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Logistic Regression - Training Accuracy: {train_acc:.4f}")
print(f"Logistic Regression - Test Accuracy: {test_acc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

# Confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Predicted 0", "Predicted 1"],
            yticklabels=["Actual 0", "Actual 1"])
plt.title("Confusion Matrix for Logistic Regression")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

"""#Part 2: Unsupervised Learning
Part A. Clustering with TF-IDF

"""

#UNSUPERVISED

from sklearn.cluster import KMeans


# Load test dataset and removng extra cols
test_df = pd.read_csv("/content/test_misinfo(in).csv").iloc[:, :2]

def preprocess_data(df):

    df = df.dropna().reset_index(drop=True)
    df["label"] = pd.to_numeric(df["label"], errors='coerce')
    df = df.dropna(subset=["label"])
    df["label"] = df["label"].astype(int)
    return df

# Preprocess dataset
test_df = preprocess_data(test_df)
y_test = test_df["label"].values

# Convert text to TF-IDF vectors
vectorizer = TfidfVectorizer(max_features=10000)
X_test_tfidf = vectorizer.fit_transform(test_df["text"])

#KMeans clustering
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
kmeans.fit(X_test_tfidf)
kmeans_labels = kmeans.labels_

# sampling 50 misinformation and 50 real news
sample_real = test_df[test_df["label"] == 0].sample(n=50, random_state=42)
sample_fake = test_df[test_df["label"] == 1].sample(n=50, random_state=42)
sample_df = pd.concat([sample_real, sample_fake])

# Convert sampled text to TF-IDF vectors
sample_vectors = vectorizer.transform(sample_df["text"])
sample_labels = sample_df["label"].values
sample_clusters = kmeans.predict(sample_vectors)

# Determine majority class
mapping = {}
for cluster in np.unique(sample_clusters):
    true_labels = sample_labels[sample_clusters == cluster]
    mapping[cluster] = np.bincount(true_labels).argmax()

# Convert cluster labels to final predictions
predicted_labels = np.array([mapping[label] for label in kmeans_labels])

# Evaluating the clusters
cluster_acc = accuracy_score(y_test, predicted_labels)
conf_matrix = confusion_matrix(y_test, predicted_labels)

print(f"KMeans (TF-IDF) Clustering Accuracy: {cluster_acc:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

#print conf matrix figure
# give labels

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",

            xticklabels=["Predicted 0", "Predicted 1"],
            yticklabels=["Actual 0", "Actual 1"])

"""Part B. Clustering with Sentence Embeddings Representations"""

from sentence_transformers import SentenceTransformer

# Load and preprocess test dataset
def preprocess_data(df):
    df = df.dropna().reset_index(drop=True)
    df["label"] = pd.to_numeric(df["label"], errors='coerce')
    df = df.dropna(subset=["label"])
    df["label"] = df["label"].astype(int)
    return df

# Load dataset and clean it
test_df = preprocess_data(pd.read_csv("/content/test_misinfo(in).csv").iloc[:, :2])
y_test = test_df["label"].values

# checking text column
if "text" not in test_df.columns:
    raise ValueError("The 'text' column is missing from the dataset!")

# Convert text column to string format
test_df["text"] = test_df["text"].astype(str)

# Load Sentence Transformer model
print("Loading SentenceTransformer model...")
model = SentenceTransformer("all-MiniLM-L6-v2")

# embeddings
X_test_embeddings = np.array(model.encode(test_df["text"].tolist()))

# KMeans clustering
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
kmeans.fit(X_test_embeddings)
kmeans_labels = kmeans.labels_

real_news = test_df[test_df["label"] == 0].reset_index(drop=True)
misinfo = test_df[test_df["label"] == 1].reset_index(drop=True)

sample_real = real_news.sample(n=min(50, len(real_news)), random_state=42)
sample_fake = misinfo.sample(n=min(50, len(misinfo)), random_state=42)

sample_df = pd.concat([sample_real, sample_fake]).reset_index(drop=True)

# Conversion of txt to embed and predictions
sample_embed_vectors = np.array(model.encode(sample_df["text"].tolist()))
sample_labels = sample_df["label"].values
sample_embed_clusters = kmeans.predict(sample_embed_vectors)

# Map cluster labels to actual labels
mapping_embed = {}
for cluster in np.unique(sample_embed_clusters):
    indices = np.where(sample_embed_clusters == cluster)[0]  # Ensure valid indexing
    true_labels = sample_labels[indices] if len(indices) > 0 else np.array([])
    mapping_embed[cluster] = np.bincount(true_labels).argmax() if len(true_labels) > 0 else 0


# Convert cluster labels to final predictions
predicted_embed_labels = np.array([mapping_embed.get(label, 0) for label in kmeans_labels])

# Evaluation of clustering performance
embed_acc = accuracy_score(y_test, predicted_embed_labels)
conf_matrix_embed = confusion_matrix(y_test, predicted_embed_labels)

# results
print(f"KMeans (Embeddings) Clustering Accuracy: {embed_acc:.4f}")
print("Confusion Matrix:")
print(conf_matrix_embed)
#display conf matrix

#display conf matrix
sns.heatmap(conf_matrix_embed, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Predicted 0", "Predicted 1"],
            yticklabels=["Actual 0", "Actual 1"])




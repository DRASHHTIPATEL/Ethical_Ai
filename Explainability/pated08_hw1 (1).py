# -*- coding: utf-8 -*-
"""PATED08_HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aODYefqfEGw5HyLq07JKRJYlPm-zEnGJ

#ETHICAL AI HW1
 By: Drashti Patel
"""

#HW1
#DRASHTI SNEHALKUMAR PATEL

# import libraries needed
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

"""DECISION TREES"""

#Defining the given function
def DT_Explain(train_file, test_file):

  # loading the data files
  #NOTE : UPLOAD THE 2 DATA EXCEL FILES LOCALLY
    train_data = pd.read_excel(train_file)
    test_data = pd.read_excel(test_file)

    features = ['salary', 'age', 'credit score', 'debt']
        # Separation of target and features as givne in question
    #  train data
    X_train = train_data[['salary', 'age', 'credit score', 'debt']]

    y_train = train_data['approved']
    # test data
    X_test = test_data[['salary', 'age', 'credit score', 'debt']]
    y_test = test_data['approved']

    #training Dtree classifier
    model1 = DecisionTreeClassifier()

    model1.fit(X_train, y_train)
#Test accuracy on train data
    y_train_pred = model1.predict(X_train)
    train_acc = accuracy_score(y_train, y_train_pred)
    # Test accuracy on test data
    y_test_pred = model1.predict(X_test)
    test_acc = accuracy_score(y_test, y_test_pred)

    #othewr metrices I printed for reference

    precision = precision_score(y_test, y_test_pred)
    recall = recall_score(y_test, y_test_pred)
    f1 = f1_score(y_test, y_test_pred)

    print("Train Accuracy:", train_acc)
    print("Test Accuracy:", test_acc)
    print("Precision:", precision)
    print("Recall:", recall)
    print("F1 Score:", f1)

        # Visualize and save the Decision Tree
    plt.figure(figsize=(model1.get_depth() * 2, model1.get_depth() * 2))
    plot_tree(
        model1,
        feature_names=features,
        class_names=['Not Approved', 'Approved'],
        filled=True,
        rounded=True,
        fontsize=10
    )
    plt.title("Decision Tree Visualization", fontsize=16)
    plt.tight_layout()
    plt.savefig("decision_tree_plot.jpg", format="jpg", dpi=300)
    plt.show()

    return train_acc, test_acc

DT_Explain("credit_train.xlsx", "credit_test.xlsx")

"""Logistic Regression"""

from sklearn.preprocessing import normalize
from sklearn.linear_model import LogisticRegression
# LR model function
def LR_Explain(train_file, test_file):
    train_data = pd.read_excel(train_file)
    test_data = pd.read_excel(test_file)
    print(train_data.describe())

# splitting of features and labels in bot train and test sets
    X_train = train_data[['salary', 'age', 'credit score', 'debt']]
    y_train = train_data['approved']
    X_test = test_data[['salary', 'age', 'credit score', 'debt']]
    y_test = test_data['approved']

    X_train_normalized = normalize(X_train, norm='l2', axis=1, copy=True, return_norm=False)
    X_test_normalized = normalize(X_test, norm='l2', axis=1, copy=True, return_norm=False)

    model2 = LogisticRegression(random_state=42)
    model2.fit(X_train_normalized, y_train)

    y_train_pred = model2.predict(X_train_normalized)
    train_acc = accuracy_score(y_train, y_train_pred)

    y_test_pred = model2.predict(X_test_normalized)
    test_acc = accuracy_score(y_test, y_test_pred)
    precision = precision_score(y_test, y_test_pred)
    recall = recall_score(y_test, y_test_pred)
    f1 = f1_score(y_test, y_test_pred)

    print(f"Train Accuracy: {train_acc}")
    print(f"Test Accuracy: {test_acc}")
    print(f"Precision on test data: {precision}")
    print(f"Recall on test data: {recall}")
    print(f"F1 Score on test data: {f1}")

    print("\nFeature Importance (Coefficients):")
    for feature, coef in zip(['salary', 'age', 'credit score', 'debt'], model2.coef_[0]):
        print(f"{feature}: {coef}")

    return train_acc, test_acc

LR_Explain("credit_train.xlsx", "credit_test.xlsx")

"""**Neural Nets**"""

import shap
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import normalize
from sklearn.inspection import permutation_importance
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def MLP_Explain(train_file, test_file):
    # data
    train_data = pd.read_excel(train_file)
    test_data = pd.read_excel(test_file)
    features = ['salary', 'age', 'credit score', 'debt']

    X_train, y_train = train_data[features], train_data['approved']
    X_test, y_test = test_data[features], test_data['approved']

    X_train_norm = normalize(X_train, norm='l2', axis=1)
    X_test_norm = normalize(X_test, norm='l2', axis=1)

    # Model training
    model3 = MLPClassifier(random_state=42, max_iter=300)
    model3.fit(X_train_norm, y_train)

    # predictions
    y_pred_train = model3.predict(X_train_norm)
    y_pred_test = model3.predict(X_test_norm)

    # EVAL metrics
    train_acc = accuracy_score(y_train, y_pred_train)
    test_acc = accuracy_score(y_test, y_pred_test)
    precision = precision_score(y_test, y_pred_test)
    recall = recall_score(y_test, y_pred_test)
    f1 = f1_score(y_test, y_pred_test)


    print(f"Train Accuracy: {train_acc}")
    print(f"Test Accuracy: {test_acc}")
    print(f"Precision: {precision}")
    print(f"Recall: {recall}")
    print(f"F1 Score: {f1}")

    # Feature Importance using Permutation Importance
    perm_importance = permutation_importance(model3, X_test_norm, y_test, n_repeats=10, random_state=42)

    print("\ Feature Importance (Permutation Importance)")
    for feature, importance in zip(features, perm_importance.importances_mean):
        print(f"{feature}: {importance:.4f}")

    plt.figure(figsize=(8, 5))
    plt.barh(features, perm_importance.importances_mean, color='skyblue')
    plt.xlabel("Permutation Importance Score")
    plt.title("Feature Importance - Permutation Importance")
    plt.savefig("permutation_importance.png", dpi=300)
    plt.show()

    # SHAP Explainability
    print("\n Explaining Model Decisions Using SHAP ")
    explainer = shap.Explainer(model3.predict_proba, X_train_norm)
    shap_values = explainer(X_test_norm)
    shap.summary_plot(shap_values[..., 1], X_test_norm, feature_names=features)  # Select class 1 for binary classification
    plt.savefig("shap_summary.png", dpi=300)
    plt.show()
    return train_acc, test_acc
MLP_Explain("credit_train.xlsx", "credit_test.xlsx")


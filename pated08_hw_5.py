# -*- coding: utf-8 -*-
"""pated08_hw 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pELpS12qfB-M1658l-IvUz3s8_spncrF
"""

#hw5
#drashti snehalkumar patel

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""PART 1 A.EXPOSING THE LEAK"""

# TRAINING THE LR MODEL
# upload the files locally
# paths
train_data_path = '/content/train_politics(in).csv'
test_data_path = '/content/test_politics(in).csv'


# ds loading
train_data = pd.read_csv(train_data_path)
test_data = pd.read_csv(test_data_path)
print(train_data.head())

#TF-IDF Vectorization
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_data['text'])
X_test = vectorizer.transform(test_data['text'])
#train lr model
model = LogisticRegression()
model.fit(X_train, train_data['label'])

#evaluation
y_pred = model.predict(X_test)
accuracy = accuracy_score(test_data['label'], y_pred)

print(f"Accuracy on original test data: {accuracy:.4f}")

from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
#calculate precision
precision = precision_score(test_data['label'], y_pred)
print(f"Precision on original test data: {precision:.4f}")
#calculate recall
recall = recall_score(test_data['label'], y_pred)
print(f"Recall on original test data: {recall:.4f}")
#F1
f1 = f1_score(test_data['label'], y_pred)
print(f"F1 Score on original test data: {f1:.4f}")
#print confusion matrix
confusion = confusion_matrix(test_data['label'], y_pred)
print("Confusion Matrix:")
print(confusion)

"""1b. demonstrate data leak

"""

# filter tweets frm tetdata with keywords amrica and change
# predic pol affiliation and display probabilities

# Define keywords for filtering tweets (Republican: "america", Democrat: "change")
keywords_republican = 'america'
keywords_democrat = 'change'

# Filter the test data for posts containing these keywords
sample_data = test_data[test_data['text'].str.contains(keywords_republican, case=False, na=False) |
                        test_data['text'].str.contains(keywords_democrat, case=False, na=False)]

# CheckING if we have any filtered data
if sample_data.empty:
    print("No matching samples found for the specified keywords.")
else:

    X_sample = vectorizer.transform(sample_data['text'])
    y_pred = model.predict(X_sample)
    y_pred_proba = model.predict_proba(X_sample)

#Saving
    output_data = {
        'Text': sample_data['text'],
        'Predicted Label': ['Democrat' if label == 1 else 'Republican' for label in y_pred],
        'Predicted Probability Democrat': [proba[1] for proba in y_pred_proba],
        'Predicted Probability Republican': [proba[0] for proba in y_pred_proba]
    }

    output_df = pd.DataFrame(output_data)

    output_file = '/content/predictions_output.csv'
    output_df.to_csv(output_file, index=False)
    #display the output
    print(output_df)

"""#PART B FIXING THE LEAK"""

import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#anonymization function
def anonymize_text(text):
    #regex
    return re.sub(r'@[\w]+', 'USER', text)


train_data = pd.read_csv(train_data_path)
test_data = pd.read_csv(test_data_path)

train_data['text'] = train_data['text'].apply(anonymize_text)
test_data['text'] = test_data['text'].apply(anonymize_text)

# Save the anonymized datasets to new files
train_data.to_csv('/content/train_politics_anonymized.csv', index=False)
test_data.to_csv('/content/test_politics_anonymized.csv', index=False)

print("Anonymized Training Data:")
print(train_data.head())

print("\nAnonymized Test Data:")
print(test_data.head())

#training a new Logistic Regression model on the anonymized data

# TF-IDF Vectorization
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_data['text'])
X_test = vectorizer.transform(test_data['text'])

model = LogisticRegression()
model.fit(X_train, train_data['label'])

#eval
y_pred = model.predict(X_test)
accuracy = accuracy_score(test_data['label'], y_pred)
print(f"\nAccuracy on Anonymized Test Data: {accuracy:.4f}")

#print precision
from sklearn.metrics import precision_score
precision = precision_score(test_data['label'], y_pred)
print(f"Precision on anonymized test data: {precision:.4f}")
#print recall
from sklearn.metrics import recall_score
recall = recall_score(test_data['label'], y_pred)
print(f"Recall on anonymized test data: {recall:.4f}")

# calc f1 score
from sklearn.metrics import f1_score
f1 = f1_score(test_data['label'], y_pred)
print(f"F1 Score on anonymized test data: {f1:.4f}")
# calc conf matrix
from sklearn.metrics import confusion_matrix
confusion = confusion_matrix(test_data['label'], y_pred)
print("Confusion Matrix:")
print(confusion)

# showing that the privacy leak is no longer present by using the same procedure as part a
# Filter the data using the same keywords and check predictions on anonymized test data

# Define keywords
keywords_republican = 'america'
keywords_democrat = 'change'

# Filter the anonymized test data for posts containing these keywords
sample_data = test_data[test_data['text'].str.contains(keywords_republican, case=False, na=False) |
                        test_data['text'].str.contains(keywords_democrat, case=False, na=False)]

if sample_data.empty:
    print("No matching samples found for the specified keywords.")
else:
    X_sample = vectorizer.transform(sample_data['text'])
    y_pred = model.predict(X_sample)
    y_pred_proba = model.predict_proba(X_sample)

    print("Predictions for selected tweets (Democrat/Republican):")
    for idx, (index, row) in enumerate(sample_data.iterrows()):
        predicted_label = 'Democrat' if y_pred[idx] == 1 else 'Republican'
        predicted_proba = y_pred_proba[idx]
        print(f"Text: {row['text']}")
        print(f"Predicted Label: {predicted_label}")
        print(f"Predicted Probabilities: {predicted_proba}")
        print("---")

# checkibng privacy leak
# Evaluate the model on anonymized test data to check if privacy leak is present
# Re-load the anonymized test data (assuming you have the test data saved after anonymization)
anonymized_test_data_path = '/content/test_politics_anonymized.csv'  # Adjust the path as needed
anonymized_test_data = pd.read_csv(anonymized_test_data_path)
keywords_republican = 'america'
keywords_democrat = 'change'

# Filtering
sample_data_anonymized = anonymized_test_data[anonymized_test_data['text'].str.contains(keywords_republican, case=False, na=False) |
                                              anonymized_test_data['text'].str.contains(keywords_democrat, case=False, na=False)]

if sample_data_anonymized.empty:
    print("No matching samples found for the specified keywords.")
else:
    # Prepare to save the predictions and probabilities to a file
    results = []

    X_sample_anonymized = vectorizer.transform(sample_data_anonymized['text'])
    y_pred = model.predict(X_sample_anonymized)
    y_pred_proba = model.predict_proba(X_sample_anonymized)

    for idx, (index, row) in enumerate(sample_data_anonymized.iterrows()):
        predicted_label = 'Democrat' if y_pred[idx] == 1 else 'Republican'
        predicted_proba = y_pred_proba[idx]

        # strong into.a dict
        results.append({
            'Text': row['text'],
            'Predicted Label': predicted_label,
            'Predicted Probability (Democrat)': predicted_proba[1],
            'Predicted Probability (Republican)': predicted_proba[0]
        })

    output_df = pd.DataFrame(results)

    # Save the results to a CSV file
    output_file_path = '/content/anonymized_predictions_output.csv'
    output_df.to_csv(output_file_path, index=False)

    print(f"Predictions saved to {output_file_path}")

"""#PART 2 BETTER ANONYMIZATION
VERIFY ACCURACY DROP BY REMOVING NAMES
"""

import pandas as pd
import re
# func defining
def remove_names(text):
    # Replace any word that starts with a capital letter with an empty string
    return re.sub(r'\b[A-Z][a-z]*\b', '', text)

train_data_path = '/content/train_politics_anonymized.csv'
test_data_path = '/content/test_politics_anonymized.csv'

train_data = pd.read_csv(train_data_path)
test_data = pd.read_csv(test_data_path)

# Apply the remove_names function to the 'text' column in both train and test data
train_data['text'] = train_data['text'].apply(remove_names)
test_data['text'] = test_data['text'].apply(remove_names)

train_data = train_data[train_data['text'].str.strip().astype(bool)]
test_data = test_data[test_data['text'].str.strip().astype(bool)]

train_data.to_csv('/content/train_politics_removed.csv', index=False)
test_data.to_csv('/content/test_politics_removed.csv', index=False)

print("Train Data After Name Removal:")
print(train_data.head())

print("\nTest Data After Name Removal:")
print(test_data.head())

#Train and test the model on the new data where names have been removed

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_data['text'])
X_test = vectorizer.transform(test_data['text'])

# Train the Logistic Regression Model
model = LogisticRegression()
model.fit(X_train, train_data['label'])

# Evaluate the model on the removed names test data
y_pred = model.predict(X_test)
accuracy = accuracy_score(test_data['label'], y_pred)

# Print the accuracy for the model on the test data with names removed
print(f"Accuracy on Removed Names Test Data: {accuracy:.4f}")
precision = precision_score(test_data['label'], y_pred)
print(f"Precision on anonymized test data: {precision:.4f}")
recall = recall_score(test_data['label'], y_pred)
print(f"Recall on anonymized test data: {recall:.4f}")
f1 = f1_score(test_data['label'], y_pred)
print(f"F1 Score on anonymized test data: {f1:.4f}")
confusion = confusion_matrix(test_data['label'], y_pred)
print("Confusion Matrix:")
print(confusion)

# smarter anonymisation
# Define a smarter anonymization function that replaces names with placeholders
def better_anonymized_text(text):
    # Find all capitalized words (potential names)
    capitalized_words = re.findall(r'\b[A-Z][a-z]*\b', text)
    for idx, word in enumerate(capitalized_words):
        text = text.replace(word, f'USER{idx+1}')
    return text

# Apply the smarter anonymization function to both training and test data
train_data['text'] = train_data['text'].apply(better_anonymized_text)
test_data['text'] = test_data['text'].apply(better_anonymized_text)

train_data.to_csv('/content/train_politics_better.csv', index=False)
test_data.to_csv('/content/test_politics_better.csv', index=False)

#preview
print("Better Anonymized Training Data:")
print(train_data.head())

print("\nBetter Anonymized Test Data:")
print(test_data.head())

#training testing

# TF-IDF Vectorization
X_train_better = vectorizer.fit_transform(train_data['text'])
X_test_better = vectorizer.transform(test_data['text'])

model.fit(X_train_better, train_data['label'])

y_pred_better = model.predict(X_test_better)
accuracy_better = accuracy_score(test_data['label'], y_pred_better)
# eval
print(f"Accuracy on Better Anonymized Test Data: {accuracy_better:.4f}")
precision_better = precision_score(test_data['label'], y_pred_better)
print(f"Precision on anonymized test data: {precision_better:.4f}")
recall_better = recall_score(test_data['label'], y_pred_better)
print(f"Recall on anonymized test data: {recall_better:.4f}")
f1_better = f1_score(test_data['label'], y_pred_better)
print(f"F1 Score on anonymized test data: {f1_better:.4f}")
confusion_better = confusion_matrix(test_data['label'], y_pred_better)
print("Confusion Matrix:")
print(confusion_better)










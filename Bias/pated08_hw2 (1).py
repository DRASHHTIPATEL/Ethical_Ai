# -*- coding: utf-8 -*-
"""PATED08_HW2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v6NQaAXhZUOkj0eVQuTzNAkEOkLnEyty
"""

#hw2:Bias #Drashti Snehalkumar Patel

"""#Case Study 1

"""

#LOCALLY UPLOAD THE FILES

import pickle
import pandas as pd
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

#Loading the trained model
with open('/content/BEST_LR_MODEL_EVER.pkl', 'rb') as f:
    clf = pickle.load(f)

#Load the dataset
df = pd.read_csv('/content/test_bias_new.csv')

# PREPARING FEATURES AND TARGET
X = df.loc[:, df.columns != 'approved']
Y = df['approved']

# Normalize the features to match the training data format
X_normalized = preprocessing.normalize(X, axis=0)

# Checking test accuracy
test_accuracy = clf.score(X_normalized, Y)
print(f"New Test Accuracy: {test_accuracy:.4f}")

# Generate confusion matrix for the entire test set
cm_total = confusion_matrix(Y, clf.predict(X_normalized))

# Visualization
plt.figure(figsize=(6, 6))
sns.heatmap(cm_total, annot=True, fmt='d', cmap='Blues', xticklabels=['Deny', 'Approve'], yticklabels=['Deny', 'Approve'])
plt.title('Confusion Matrix - Overall Test Set')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Analyze Credit Score Bias
def analyze_credit_score_bias(df, clf):
    # Split the dataset by credit score (650 threshold)
    high_credit = df[df['credit score'] > 650]
    low_credit = df[df['credit score'] <= 650]

    # Normalize features for both high and low credit score groups
    X_high_credit = preprocessing.normalize(high_credit.loc[:, high_credit.columns != 'approved'], axis=0)
    X_low_credit = preprocessing.normalize(low_credit.loc[:, low_credit.columns != 'approved'], axis=0)

    # Single querying for first 10 high and low credit score applicants
    high_credit_predictions = [clf.predict(X_high_credit[i].reshape(1, -1))[0] for i in range(10)]
    low_credit_predictions = [clf.predict(X_low_credit[i].reshape(1, -1))[0] for i in range(10)]

    print("Predictions for High Credit Score Applicants (First 10):", high_credit_predictions)
    print("Predictions for Low Credit Score Applicants (First 10):", low_credit_predictions)

    # Make predictions for both subsets
    Y_pred_high_credit = clf.predict(X_high_credit)
    Y_pred_low_credit = clf.predict(X_low_credit)

    # Confusion matrix genertion
    cm_high_credit = confusion_matrix(high_credit['approved'], Y_pred_high_credit)
    cm_low_credit = confusion_matrix(low_credit['approved'], Y_pred_low_credit)
    #print confusion metrics
    print("Confusion Matrix for High Credit Score Applicants:")
    print(cm_high_credit)

    print("\nConfusion Matrix for Low Credit Score Applicants:")
    print(cm_low_credit)

    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    sns.heatmap(cm_high_credit, annot=True, fmt='d', cmap='Blues', xticklabels=['Deny', 'Approve'], yticklabels=['Deny', 'Approve'])
    plt.title('Confusion Matrix - High Credit Score Applicants')
    plt.xlabel('Predicted')
    plt.ylabel('True')

    plt.subplot(1, 2, 2)
    sns.heatmap(cm_low_credit, annot=True, fmt='d', cmap='Blues', xticklabels=['Deny', 'Approve'], yticklabels=['Deny', 'Approve'])
    plt.title('Confusion Matrix - Low Credit Score Applicants')
    plt.xlabel('Predicted')
    plt.ylabel('True')

    plt.tight_layout()
    plt.show()

# Analyze bias for credit score
analyze_credit_score_bias(df, clf)

# Analyze Gender Bias
def analyze_gender_bias(df, clf):
    # Split the dataset by gender
    subset_male = df[df['sex'] == 0]  # Male applicants
    subset_female = df[df['sex'] == 1]  # Female applicants

    # Normalize features for both male and female groups
    X_male = preprocessing.normalize(subset_male.loc[:, subset_male.columns != 'approved'], axis=0)
    X_female = preprocessing.normalize(subset_female.loc[:, subset_female.columns != 'approved'], axis=0)

    # Single querying for first 10 male and female applicants
    male_predictions = [clf.predict(X_male[i].reshape(1, -1))[0] for i in range(10)]
    female_predictions = [clf.predict(X_female[i].reshape(1, -1))[0] for i in range(10)]

    print("Predictions for Male Applicants (First 10):", male_predictions)
    print("Predictions for Female Applicants (First 10):", female_predictions)

    # Make predictions for both subsets
    Y_pred_male = clf.predict(X_male)
    Y_pred_female = clf.predict(X_female)

    # Generate confusion matrices
    cm_male = confusion_matrix(subset_male['approved'], Y_pred_male)
    cm_female = confusion_matrix(subset_female['approved'], Y_pred_female)
    #print confusion matrix
    print("Confusion Matrix for Male Applicants:")
    print(cm_male)

    print("\nConfusion Matrix for Female Applicants:")
    print(cm_female)
    # Visualize the confusion matrices
    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    sns.heatmap(cm_male, annot=True, fmt='d', cmap='Blues', xticklabels=['Deny', 'Approve'], yticklabels=['Deny', 'Approve'])
    plt.title('Confusion Matrix - Male Applicants')
    plt.xlabel('Predicted')
    plt.ylabel('True')

    plt.subplot(1, 2, 2)
    sns.heatmap(cm_female, annot=True, fmt='d', cmap='Blues', xticklabels=['Deny', 'Approve'], yticklabels=['Deny', 'Approve'])
    plt.title('Confusion Matrix - Female Applicants')
    plt.xlabel('Predicted')
    plt.ylabel('True')

    plt.tight_layout()
    plt.show()

# we move further by checking other bias, ie in sex .. intially we do single querying , I decide randomly to check for 10 from each category

# Analyze bias for gender
analyze_gender_bias(df, clf)

# BIAS IS PRESENT IN GENDER

# WE CHECK THE TRAINING DS OR TEST DATA DONE BY BOSS JUST FOR REFERENCE PURPOSES

# Step 2: Load the original dataset (test_bias.csv)
original_df = pd.read_csv('/content/test_bias.csv')

# Step 3: Split the data into male and female groups based on the 'sex' column
subset_male_original = original_df[original_df['sex'] == 0]  # Male applicants
subset_female_original = original_df[original_df['sex'] == 1]  # Female applicants

# Step 4: Normalize the features for both male and female groups
X_male_original = preprocessing.normalize(subset_male_original.loc[:, subset_male_original.columns != 'approved'], axis=0)
X_female_original = preprocessing.normalize(subset_female_original.loc[:, subset_female_original.columns != 'approved'], axis=0)

# Step 5: Make predictions for both subsets
Y_pred_male_original = clf.predict(X_male_original)
Y_pred_female_original = clf.predict(X_female_original)

# Step 6: Generate confusion matrices for male and female groups
cm_male_original = confusion_matrix(subset_male_original['approved'], Y_pred_male_original)
cm_female_original = confusion_matrix(subset_female_original['approved'], Y_pred_female_original)

# Step 7: Visualize the confusion matrices
plt.figure(figsize=(12, 6))

# Confusion matrix for male applicants
plt.subplot(1, 2, 1)
sns.heatmap(cm_male_original, annot=True, fmt='d', cmap='Blues', xticklabels=['Deny', 'Approve'], yticklabels=['Deny', 'Approve'])
plt.title('Confusion Matrix - Male Applicants (Original Dataset)')
plt.xlabel('Predicted')
plt.ylabel('True')

# Confusion matrix for female applicants
plt.subplot(1, 2, 2)
sns.heatmap(cm_female_original, annot=True, fmt='d', cmap='Blues', xticklabels=['Deny', 'Approve'], yticklabels=['Deny', 'Approve'])
plt.title('Confusion Matrix - Female Applicants (Original Dataset)')
plt.xlabel('Predicted')
plt.ylabel('True')

plt.tight_layout()
plt.show()





"""#Case Study 2

Locating bias
1.check class distribution
2. conf matrix
3.feature influence
"""

# locally upload all files needed

import pickle
import pandas as pd

# Load the trained model
with open('/content/PassPredictionLR.pkl', 'rb') as f:
    clf = pickle.load(f)

# Load the training and test datasets
train_df = pd.read_csv('/content/passPred_train.csv')  # Training data
test_df = pd.read_csv('/content/passPred_test.csv')  # Test data

import pickle
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the trained model
with open('/content/PassPredictionLR.pkl', 'rb') as f:
    clf = pickle.load(f)

# Load the dataset
train_df = pd.read_csv('/content/passPred_train.csv')
test_df = pd.read_csv('/content/passPred_test.csv')

#print entire dataset

print(train_df)
print(test_df)

# Prepare features and target variables
X_train = train_df[['average_HW_score', 'exam1_score', 'attendance_percentage']]
y_train = train_df['will_pass']
X_test = test_df[['average_HW_score', 'exam1_score', 'attendance_percentage']]
y_test = test_df['will_pass']

# Normalize the features
X_train_normalized = preprocessing.normalize(X_train, axis=0)
X_test_normalized = preprocessing.normalize(X_test, axis=0)

# Calculate accuracy for the initial model
train_accuracy = clf.score(X_train_normalized, y_train)
test_accuracy = clf.score(X_test_normalized, y_test)

print(f"Train Accuracy: {train_accuracy:.2f}")
print(f"Test Accuracy: {test_accuracy:.2f}")

# Calculate and display the summary statistics (min, max, mean, std, etc.)
print("Feature Summary Statistics:")
print(train_df[['average_HW_score', 'exam1_score', 'attendance_percentage']].describe())
print(test_df[['average_HW_score', 'exam1_score', 'attendance_percentage']].describe())

# Check the class distribution in both training and testing datasets
print("Training Data Class Distribution:\n", y_train.value_counts())
print("Test Data Class Distribution:\n", y_test.value_counts())

# Generate confusion matrix for the test set
y_pred = clf.predict(X_test_normalized)
cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])
plt.title('Confusion Matrix - Test Set')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

#the model is over predicting pass ,ie , all students are being predicted as "pass", regardless of whether they actually passed or failed.

#The model is likely failing to learn to predict the minority class ("fail") because of class imbalance. Since the majority of the training data is made up of "pass" students, the model has learned to predict "pass" for all cases, even if the actual outcome should be "fail".
#This kind of behavior usually happens when the model is not penalized enough for misclassifying the minority class (the "fail" students). The class imbalance leads the model to ignore the minority class entirely.

# Get the coefficients of the trained logistic regression model
coefficients = pd.DataFrame({
    'Feature': X_train.columns,
    'Coefficient': clf.coef_[0]
})

# Display the coefficients
print(coefficients)

# no clear bias being detected using coeffs , hence we move ahead with class imbalance

#MITIGATION

import pickle
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt

# Apply SMOTE to oversample the minority class in the training data
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Normalize the features after SMOTE
X_train_smote_normalized = preprocessing.normalize(X_train_smote, axis=0)
X_test_normalized = preprocessing.normalize(X_test, axis=0)  # Normalize test set as well

# Train the Logistic Regression model on the oversampled and normalized data
clf_smote = LogisticRegression(max_iter=1000)
clf_smote.fit(X_train_smote_normalized, y_train_smote)

# Calculate accuracy for the new model
train_accuracy_smote = clf_smote.score(X_train_smote_normalized, y_train_smote)
test_accuracy_smote = clf_smote.score(X_test_normalized, y_test)

print(f"Train Accuracy (SMOTE): {train_accuracy_smote:.2f}")
print(f"Test Accuracy (SMOTE): {test_accuracy_smote:.2f}")

# Generate confusion matrix for the test set
y_pred_smote = clf_smote.predict(X_test_normalized)
cm_smote = confusion_matrix(y_test, y_pred_smote)
#print confusion matrix
print("Confusion Matrix - Test Set (SMOTE):")
print(cm_smote)

# Visualize the confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm_smote, annot=True, fmt='d', cmap='Blues', xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])
plt.title('Confusion Matrix - Test Set (SMOTE)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()



#Smote is also not giving required rsults
 # thus we can try using a standardisation tehnique for more better results and to reduce the feature scale bias. thus we use min max scaling

import pickle
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Load the trained model
with open('/content/PassPredictionLR.pkl', 'rb') as f:
    clf = pickle.load(f)

# Load the dataset
train_df = pd.read_csv('/content/passPred_train.csv')
test_df = pd.read_csv('/content/passPred_test.csv')

# Prepare features and target variables
X_train = train_df[['average_HW_score', 'exam1_score', 'attendance_percentage']]
y_train = train_df['will_pass']
X_test = test_df[['average_HW_score', 'exam1_score', 'attendance_percentage']]
y_test = test_df['will_pass']

# Apply SMOTE to oversample the minority class in the training data
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Normalize the features after SMOTE using Min-Max scaling
scaler = MinMaxScaler()
X_train_smote_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)  # Normalize the test data with the same scaler

# Train the Logistic Regression model on the oversampled and scaled data
clf_smote = LogisticRegression(max_iter=1000)
clf_smote.fit(X_train_smote_scaled, y_train_smote)

# Calculate accuracy for the new model
train_accuracy_smote = clf_smote.score(X_train_smote_scaled, y_train_smote)
test_accuracy_smote = clf_smote.score(X_test_scaled, y_test)

print(f"Train Accuracy (SMOTE + Min-Max Scaling): {train_accuracy_smote:.2f}")
print(f"Test Accuracy (SMOTE + Min-Max Scaling): {test_accuracy_smote:.2f}")

# Generate confusion matrix for the test set
y_pred_smote = clf_smote.predict(X_test_scaled)
cm_smote = confusion_matrix(y_test, y_pred_smote)
#print confusion matrix
print("Confusion Matrix - Test Set (SMOTE + Min-Max Scaling):")
print(cm_smote)

# Visualize the confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(cm_smote, annot=True, fmt='d', cmap='Blues', xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])
plt.title('Confusion Matrix - Test Set (SMOTE + Min-Max Scaling)')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

